\section{Principes}

L’approche \gls{svm} est un ensemble de méthodes supervisées utilisant :
\begin{enum}
\item{un \gls{ds} d’apprentissage pour entraîner l’algorithme,\\
et qui fait donc office de superviseur}
\item{un \gls{ds} de test pour vérifier sa pertinence}
\end{enum}

Cette approche se révèle appropriée dans de nombreux cas d’utilisation :
\begin{itmz}
\item{filtrage d’email, courriel légitime ou pourriel (phishing, spam)}
\item{classification d’images, quel que soit le \gls{si}}
\item{détection de \gls{sgn} dans des fichiers multimédias}
\item{quantification de granularité dans des textures}
\item{reconnaissance de caractères et d’écriture dans des images}
\item{classification d’expressions faciales dans des images}
\item{reconnaissance vocale dans des échantillons sonores}
\item{classification et prédiction de structure de protéines}
\item{établissement de diagnostics médicaux}
\item{classification de documents texte en différentes catégories}
\end{itmz}

En fonction du type de problèmes, deux types de résolution :
\begin{itmz}
\item{\textbf{régression} (\gls{svr}) → nombre}
\item{\textbf{classification} (\gls{svc}) → catégorie}
\end{itmz}

En fonction des \glspl{ds}, deux types d’approche mathématique :
\begin{itmz}
\item{\textbf{linéaire} : la plus simple}
\item{\textbf{non linéaire} : faisant appel à des \glspl{kf}}
\end{itmz}

Quatre paramètres permettent d’affiner le modèle :
\begin{itmz}
\item{\textbf{noyau} : linéaire, \gls{rbf}, polynomial, \gls{sigmoid}, etc.}
\item{\textbf{degré} : aide à trouver un \gls{hpp} séparateur en contexte polynomial,
faisant rapidement augmenter le temps nécessaire à l’entraînement}
\item{\textbf{γ} : pour les \glspl{hpp} non linéaires}
\item{\textbf{C} : pénalité augmentant la distance des données prises en compte, au\\
risque d’engendrer un surentraînement pour une valeur trop importante}
\end{itmz}

\pagebreak

\subsection{Régression}

Un hyperparamètre \textbf{ε} permet de fait varier l’épaisseur de la marge,
pour y inclure le plus de données possible.
Les éléments exclus sont identifiés en rose.

\subsubsection{Régression linéaire}

Régression la plus simple : une approximation affine est suffisante.

\bifig{}{Régression linéaire, variation d’ε \cite{homl-linear}}
{15em}{regression_linear_left}{regression_linear_right}

\subsubsection{Régression non linéaire}

Régression nécessitant l’utilisation d’une fonction noyau.\\
Une plus grande valeur de C intègre des données plus éloignées.

\bifig{}{Régression polynomiale de degré 2, variation de C \cite{homl-nonlinear}}
{15em}{regression_nonlinear_left}{regression_nonlinear_right}

\pagebreak

\subsection{Classification}

Il s’agit du type de résolution le plus fréquemment utilisé.

\subsubsection{Classification linéaire}

Cette section se penche sur la classification de 2 espèces d’iris,
en fonction des longueurs et largeurs de leurs pétales.

\textbf{Séparation à Vaste Marge}

La figure de gauche montre que dans l’absolu, un grand nombre de droites
peut séparer correctement les 2 ensembles à classifier.
La figure de droite montre cependant qu’en utilisant les éléments
les plus proches, appelés dans ce cas \glspl{sv}, il est alors possible
de définir une marge de séparation la plus large qui soit, afin de
déterminer la droite médiane de séparation la plus efficace.

\bifig{}{Séparation à Vaste Marge \cite{homl-large-scale}}
{9em}{margin_large_left}{margin_large_right}

Un changement d’échelle préalable aide à la séparation des données,
et peut mener à une meilleure efficacité du modèle pour la classification.
\cite{scaling}

La figure de droite montre l’inclusion d’un \gls{sv} supplémentaire.

\bifig{}{Changements d’échelles de dimensions \cite{homl-large-scale}}
{10em}{margin_scale_left}{margin_scale_right}

\pagebreak

\textbf{Séparation à marge souple}

L’approche de vaste marge peut être perturbée par 2 problématiques distinctes.

La figure de droite montre par exemple des \glspl{sv} tellement proches,
que la pertinence du modèle s’en trouve forcément impactée, réduisant
ainsi la fiabilité de la séparation.

La figure de gauche montre quant à elle une anomalie (outlier),
rendant de fait toute séparation linéaire impossible.

\bifig{}{Sensibilité de vaste marge aux anomalies \cite{homl-hard-few}}
{9.5em}{margin_hard_left}{margin_hard_right}

Il faut donc utiliser un modèle plus flexible pour pouvoir éviter
ce type de problèmes. Le but étant de trouver le meilleur compromis
entre avoir la marge la plus large, et y retrouver le moins possible
d’éléments intrus, appelés violations de marge.

Il est ainsi possible d’utiliser l’hyperparamètre \textbf{C} pour faire
varier la distance de prise en compte des éléments proches de la marge.

Plus la valeur de \textbf{C} augmente, plus le nombre de violations de
marge diminiue, mais plus le modèle se spécialise.

Une attention particulière doit donc être portée à la réduction de marge,
sans quoi le modèle perdrait en capacité de généralisation
et donc en précision.

\bifig{}{Plus ou moins de violations de marge, variation de C \cite{homl-hard-few}}
{9.5em}{margin_few_left}{margin_few_right}

\pagebreak

\subsubsection{Classification non linéaire}

Toutes les données ne sont pas forcément séparables de façon linéaire.

Pour y adapter un modèle, il est donc nécessaire de passer par la création
de nouvelles variables résultant de la transformation des données.

Sur la ligne de la figure de gauche, des éléments verts sont entourés d’éléments bleus, inséparables linéairement tels quels.

La figure de droite introduit alors une variable $X_{2}$, élévation
au carré de la variable d’origine $X_{1}$, permettant ainsi
une séparation linéaire des données.

\bifig{}{Séparation linéaire par ajout de variable \cite{homl-nonlinear-linear}}
{14.5em}{nonlinear_linear_left}{nonlinear_linear_right}

L’ajout de variables polynomiales autorise donc des séparations sous
forme de courbes, et non plus seulement de droites.

La figure suivante montre un exemple de séparation curviligne régulière,
correspondant à la répartition des 2 catégories présentes.

\fig{}{Classification utilisant des variables polynomiales \cite{homl-feat-poly}}
{14em}{features_polynomial}

\pagebreak

\textbf{Noyau polynomial}

Combiner des polynomes de degrés faibles est simple, pratique et
relativement rapide à calculer.

Mais plus les \glspl{ds} seront complexes, plus il faudra composer avec
des polynomes de plus haut degré, générant un grand nombre de variables,
et rendant ainsi le modèle trop lent.

Un outil mathématique appelé \gls{kt} permet de contourner ce problème.
Se basant sur le théorème de Mercer, il permet de remplacer dans un espace
de grande dimension, un produit scalaire par une \gls{kf} facile à calculer.

Ce \gls{kt} simplifie donc les calculs ultérieurs d’affinage.
La technique \gls{gs}, limitant les hyperparamètres à des sous-ensembles
de valeurs prédéfinies respectant un certain pas, permet ensuite un bon
compromis de temps de calcul pour trouver les valeurs de réglages
les plus appropriées.

\bifig{}{\Gls{kf} polynomiale \cite{homl-poly}}
{14em}{kernel_polynomial_left}{kernel_polynomial_right}

\textbf{Similarité}

Une autre façon de gérer des données non linéairement séparables
est d’utiliser une fonction de similarité, comme la \gls{rbf} gaussienne.

Le principe est de transformer les éléments en fonction de leur similitude
avec des points de repères déterminés dans l’ensemble d’origine.
Utiliser chacun des éléments comme point de repère augmente la probabilité
de séparabilité linéaire, mais également le nombre de variables et donc
le temps de calcul.

La figure de gauche montre les 2 points de repère choisis en rouge,
ainsi que leur courbe gaussienne associée.

La figure de droite montre tous les points transformés avec la \gls{rbf}
gaussienne, devenant également linéairement séparables.

\bifig{}{Variables de similarité utilisant la \gls{rbf} gaussienne \cite{homl-feat-simi}}
{14em}{features_similar_left}{features_similar_right}

\textbf{Noyau gaussien \gls{rbf}}

Le \gls{kt} marche également dans un tel contexte de similarité.

Faire varier \textbf{C} (colonnes) change la distance de prise en compte.\\
Faire varier \textbf{γ} (rangées) modifie l’épaisseur de la « cloche » gaussienne.

\bifig{}{\Gls{kf} gaussienne (\gls{rbf}) \cite{homl-rbf}}
{26em}{kernel_rbf_left}{kernel_rbf_right}

\pagebreak

\textbf{Autres noyaux}

En complément aux plus courants vus précédemment, un certain nombre
d’autres noyaux existe :

\begin{itmz}
\item{\gls{rbf} de Laplace}
\item{tangente hyperbolique}
\item{sigmoïde}
\item{fonction de Bessel de première espèce}
\item{\gls{rbf} \gls{anova}}
\item{sillon linéaire à 1 dimension}
\item{chaîne (utilisé pour les documents texte ou séquences d’ADN)}
\end{itmz}

\textbf{Autres outils}

Des algorithmes récents se sont montrés plus efficaces avec de grands \glspl{ds} :

\begin{itmz}
\item{descente par sous-gradient, avec des techniques de décomposition}
\item{descente par coordonnée, en effectuant des minimisations itératives}
\end{itmz}

\subsubsection{Classification multi-classes}

Quand au moins 3 catégories sont à classifier,
et donc pour étendre les modèles à plus de 2 catégories,
deux approches sont intéressantes \cite{multi-class} :

\begin{itmz}
\item{\gls{svm} à arbre de décision}
\item{\gls{svm} par paires}
\end{itmz}

\subsubsection{Optimisation}

Normaliser les données avant de les classifier peut donner de bien meilleurs
résultats en fonction du type de problèmes considéré.

Dans le cadre de la proposition d’un nouvel algorithme pour classifier des
tumeurs cérébrales à partir d’\gls{irm} \cite{mri}, sont recensées
(entre 1990 et 2018) de nombreuses combinaisons de pré-traitements
et de classifications. \cite{optimization}

Parmi ces traitements : filtre gaussien, réduction de bruit,
suppression d’artefacts, égalisation d’histogramme, filtre médian.

\pagebreak
